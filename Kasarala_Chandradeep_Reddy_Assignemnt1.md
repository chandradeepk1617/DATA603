## 1) WHAT IS BIGDATA AND ITS TYPES

Big data refers to data collections that are too vast or complicated for conventional data-processing application software to handle. Huge amount of data is generated from a variety of sources like mobile devices, inexpensive and widespread Internet of Things devices, aerial (remote sensing), software logs, cameras, microphones, radio-frequency identification (RFID) readers, and wireless sensor networks. 


### Types of bigdata. 

Big data is classified into 3 types
1.	Structured
2.	Semi Structured
3.	UnStructured
### 1.Structured Data :

Structured data is easy to analyze and sort because it has predefined properties and is present in structured or tabular schema. Each field is independent and accessible separately or in combinations with data from other fields because it is predefined. Excel files and Sql databases are examples of structured data.

### 2.Unstructured Data :
Unstructured data consists of information without conceptual definitions that is difficult for typical databases or data models to interpret or analyze. Big data is mostly composed of unstructured data, which includes facts, dates, and numbers. The photos we upload on Facebook and other social network platforms are examples of Unstructured data.

### 3.SemiStructured Data :
Semi Structured data is a combination of structured and unstructured data. This indicates that while it shares some traits with structured data, it also contains data that lacks a clear organization and does not follow relational databases.   Examples of semi-structured data are JSON and XML, for instance.


## 2)  6 V's Of Big Data

### Volume :
Volume describes the amount of data that is produced and stored in a big data system. We're discussing data that is very huge and does not fit in a single machine and needs big data tools to process it.

### Velocity :
Velocity refers to the speed at which the data is coming. The rate at which data accumulates also affects whether the data is categorized as big data or regular data. Systems must be capable of handling the rate and volume of data creation since much of this data must be evaluated in real-time.

### Variety :
Variety includes the different formats and ways that different sorts of data are set up and prepared for processing. Big names like Facebook, Twitter, Pinterest, Google Ads, and CRM systems generate data that may be gathered, saved, and then evaluated.

### Veracity : 
Veracity reveals the quality and source of data, enables it to be viewed as dubious, contradictory, or impure, and offers information on issues you're unsure how to handle. The data's veracity and authenticity, and what can you do with it, in brief. It is, in a sense, a hygiene issue. By demonstrating the accuracy of your data, you demonstrate that you have carefully examined it.

### Variability: 
Data variability, commonly referred to as spread or dispersion, describes how dispersed a set of data is. Users can use statistics to compare their data to other sets of data by using the concept of variation, which gives users a way to explain how much data sets differ from one another.

### Value :
This V explains the value that each type of data may provide as well as how big data improves the outcomes of previously recorded data. The amount of data that we store or process is essential, but it is not the sole factor. It is also data that must be stored, analyzed, and reviewed in order to gain insights. It is data that is valuable and trustworthy.


## 3) PHASES OF DATA ANALYSIS

### Phase 1 Data Acquisition and Recording :
Data is generated from so many sources like cameras,social networkposts etc. But all the data that is generated is of no use and we need to write some filters to filter the data.

### Phase 2: Information Extraction and Cleaning :
In this phase we need to extract the use information from the generated data. We need to clean the data like filling the missing values, dropping the unnecessary columns etc.

### Phase 3: Data Integration, Aggregation, and Representation:
In this phase we need to merge information that we get from various heterogenous sources. The data that is extracted can be visualized using different visualization tools like tableau, power BI etc

### Phase 4: Query Processing, Data Modeling, and Analysis:
The simplest technique to get usable data subsets from a larger collection of data is using a query. An abstract model known as a data model organizes data items and standardizes how they relate to one another. Eg; ER Model , Semantic Data Model. The data needs to be analysed and all the times the sql queries

### Phase 5: Interpretation :
In this phase we need to understand and verify the results that are produced. We might need to know some preliminary information like how the data is derived and on what inputs we got the results etc. By this phase the model is ready for decision making.


## 4) Challenges of Big Data Analysis 

### Heterogenity  : 
The biggest challenge of the big data analysis is the heterogenous data. Even though we clean the data some times some heterogenous data might be left in the dataset. Even this small heterogenous data will lead to the failure of the model.

### Scale : 
Data Volume is increasing quickly than the computer resources. There will be some times where we require more resources and there are some times where we require less resources. For example during the black Friday sales or any other sale the traffic on the sites is higher so more resources must be allocated. Utilising a cloud computing model will be a promising solution.

### Timeliness : 
Timeliness is  one of the major challenges . There will be times where the data will be too large and the results must be delivered immediately . We need to be accurate while delivering the results in time.

### Privacy :  
One of the major difficulties of big data is keeping these enormous sets of data secure. Companies frequently put data security to later phases because they are so busy understanding, storing, and analyzing their data sets. However, this is a bad idea because unprotected data repositories might serve as a haven for malevolent hackers.

#### Human Collobaration : 
Human analysis is also required in the big data analysis along with the computational analysis . Big data system must support input from various human experts. We need to be deal with the errors that might be possible. 


## Referenece :
### https://bau.edu/blog/characteristics-of-big-data/
### https://www.upgrad.com/blog/major-challenges-of-big-data/
### https://en.wikipedia.org/wiki/Big_data
### Class notes


```python

```
